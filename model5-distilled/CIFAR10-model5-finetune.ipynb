{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyNe5REw+ENG/VLR7o3w9x0O"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "osZulJYZa-Nx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from google.colab import drive\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "DRIVE_CHECKPOINT_DIR = '/content/drive/MyDrive/CIFAR10-model5-training'\n",
    "# os.makedirs(DRIVE_CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        # conv\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\n",
    "        # classification\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])"
   ],
   "metadata": {
    "id": "Lxx_CuhzbArT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_data_loaders(batch_size=128, num_workers=2):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    return trainloader, testloader, classes\n",
    "\n"
   ],
   "metadata": {
    "id": "BnafGNulbXfo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model, trainloader, criterion, optimizer, scheduler, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loop = tqdm(trainloader, desc=f\"Epoch {epoch + 1}\")\n",
    "    for batch_idx, (inputs, targets) in enumerate(loop):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        loop.set_postfix(loss=train_loss / (batch_idx + 1), acc=100. * correct / total)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(trainloader), 100. * correct / total\n",
    "\n",
    "\n",
    "def val(model, testloader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in testloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return test_loss / len(testloader), 100.0 * correct / total\n",
    "\n",
    "\n",
    "def main():\n",
    "    num_epochs = 100\n",
    "    batch_size = 128\n",
    "    lr = 0.05\n",
    "    T_0 = 10\n",
    "    T_mult = 2\n",
    "\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "\n",
    "    trainloader, testloader, classes = get_data_loaders(batch_size)\n",
    "\n",
    "    model = ResNet152().to(device)\n",
    "    print(f\"Created ResNet152 model with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=T_0, T_mult=T_mult, eta_min=1e-5)\n",
    "\n",
    "    start_epoch = 0\n",
    "    if os.path.exists('/content/drive/MyDrive/CIFAR10-model5-training/resnet152_checkpoint_epoch_69.pt'):\n",
    "        checkpoint = torch.load('/content/drive/MyDrive/CIFAR10-model5-training/resnet152_checkpoint_epoch_69.pt')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Resuming from epoch {start_epoch}\")\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    best_acc = 0\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        train_loss, train_acc = train(model, trainloader, criterion, optimizer, scheduler, epoch)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        test_loss, test_acc = val(model, testloader, criterion)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "        print(f'\\nEpoch {epoch + 1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'test_loss': test_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'test_acc': test_acc\n",
    "        }, 'results/resnet152_checkpoint.pt')\n",
    "\n",
    "        # Save checkpoint to Google Drive\n",
    "        drive_checkpoint_path = f'{DRIVE_CHECKPOINT_DIR}/resnet152_checkpoint_epoch_{epoch+1}.pt'\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'test_loss': test_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'test_acc': test_acc\n",
    "        }, drive_checkpoint_path)\n",
    "        print(f'Checkpoint saved to Google Drive: resnet152_checkpoint_epoch_{epoch+1}.pt')\n",
    "\n",
    "        latest_drive_checkpoint_path = f'{DRIVE_CHECKPOINT_DIR}/resnet152_checkpoint_latest.pt'\n",
    "        shutil.copy2(drive_checkpoint_path, latest_drive_checkpoint_path)\n",
    "\n",
    "        # Save best model\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            # Save to local directory\n",
    "            torch.save(model.state_dict(), 'results/resnet152_best.pt')\n",
    "\n",
    "            # Save to Google Drive\n",
    "            best_drive_path = f'{DRIVE_CHECKPOINT_DIR}/resnet152_best.pt'\n",
    "            torch.save(model.state_dict(), best_drive_path)\n",
    "            print(f'New best model saved to Google Drive! (Accuracy: {test_acc:.2f}%)')\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            lrs = scheduler.get_last_lr()\n",
    "            print(f'Current LR: {lrs[0]:.6f}')\n",
    "\n",
    "        if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:\n",
    "            plt.figure(figsize=(12, 5))\n",
    "\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(train_losses, label='Train Loss')\n",
    "            plt.plot(test_losses, label='Test Loss')\n",
    "            plt.title('Loss vs. Epochs')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(train_accs, label='Train Accuracy')\n",
    "            plt.plot(test_accs, label='Test Accuracy')\n",
    "            plt.title('Accuracy vs. Epochs')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Accuracy (%)')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'results/training_curves_epoch_{epoch + 1}.png')\n",
    "            plt.close()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.title('Loss vs. Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Train Accuracy')\n",
    "    plt.plot(test_accs, label='Test Accuracy')\n",
    "    plt.title('Accuracy vs. Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/final_training_curves.png')\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Best accuracy: {best_acc:.2f}%')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "metadata": {
    "id": "jaraEha3bZhT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_model(modelPath, outputDir='./', max_examples=100):\n",
    "    if not os.path.exists(outputDir):\n",
    "        os.makedirs(outputDir)\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    model = ResNet152()\n",
    "    model.load_state_dict(torch.load(modelPath, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    misclassified_images = []\n",
    "    misclassified_predictions = []\n",
    "    misclassified_labels = []\n",
    "    misclassified_confidences = []\n",
    "\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(testloader, desc='Evaluating'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "            misclassified_indices = (predicted != labels).nonzero(as_tuple=True)[0]\n",
    "\n",
    "            for idx in misclassified_indices:\n",
    "                if len(misclassified_images) < max_examples:  # Limit number of examples\n",
    "                    misclassified_images.append(images[idx].cpu())\n",
    "                    misclassified_predictions.append(predicted[idx].item())\n",
    "                    misclassified_labels.append(labels[idx].item())\n",
    "\n",
    "                    softmax_probs = torch.nn.functional.softmax(outputs[idx], dim=0)\n",
    "                    confidence = softmax_probs[predicted[idx]].item()\n",
    "                    misclassified_confidences.append(confidence)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    print(f'Saving {len(misclassified_images)} misclassified examples to {outputDir}...')\n",
    "\n",
    "    with open(f'{outputDir}/misclassifications_summary.txt', 'w') as f:\n",
    "        f.write(f'Total accuracy: {accuracy:.2f}%\\n')\n",
    "        f.write(f'Total misclassifications: {total - correct} out of {total}\\n\\n')\n",
    "        f.write('Class-wise misclassifications:\\n')\n",
    "\n",
    "        class_correct = [0] * 10\n",
    "        class_total = [0] * 10\n",
    "\n",
    "        for i in range(len(all_targets)):\n",
    "            label = all_targets[i]\n",
    "            class_total[label] += 1\n",
    "            if all_predictions[i] == label:\n",
    "                class_correct[label] += 1\n",
    "\n",
    "        for i in range(10):\n",
    "            class_acc = 100 * class_correct[i] / class_total[i]\n",
    "            f.write(f'{classes[i]}: {class_acc:.2f}% correct ({class_correct[i]} out of {class_total[i]})\\n')\n",
    "\n",
    "        f.write('\\nTop confusion pairs (true → predicted):\\n')\n",
    "        confusion = {}\n",
    "        for true, pred in zip(all_targets, all_predictions):\n",
    "            if true != pred:\n",
    "                key = (true, pred)\n",
    "                confusion[key] = confusion.get(key, 0) + 1\n",
    "                confusion[key] = confusion.get(key, 0) + 1\n",
    "\n",
    "        top_confusions = sorted(confusion.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        for (true, pred), count in top_confusions:\n",
    "            f.write(f'{classes[true]} → {classes[pred]}: {count} times\\n')\n",
    "\n",
    "    ncols = 5\n",
    "    nrows = min(len(misclassified_images) // ncols + (1 if len(misclassified_images) % ncols != 0 else 0), 20)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 3 * nrows))\n",
    "    fig.suptitle(f'Misclassified Examples (showing {nrows * ncols} out of {len(misclassified_images)})', fontsize=16)\n",
    "\n",
    "    def denormalize(tensor):\n",
    "        mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)\n",
    "        return tensor * std + mean\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < len(misclassified_images):\n",
    "            img = denormalize(misclassified_images[i])\n",
    "            img = np.transpose(img.numpy(), (1, 2, 0))  # Convert to HWC format\n",
    "            img = np.clip(img, 0, 1)  # Ensure values are between 0 and 1\n",
    "\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(\n",
    "                f'True: {classes[misclassified_labels[i]]}\\nPred: {classes[misclassified_predictions[i]]}\\nConf: {misclassified_confidences[i]:.2f}')\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.savefig(f'{outputDir}/misclassified_examples.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    print(f'Misclassifications summary saved to {outputDir}/misclassifications_summary.txt')\n",
    "    print(f'Misclassified examples grid saved to {outputDir}/misclassified_examples.png')\n",
    "\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "\n",
    "    cm = confusion_matrix(all_targets, all_predictions)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(f'{outputDir}/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    class_accuracies = [100 * class_correct[i] / class_total[i] for i in range(10)]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(classes, class_accuracies)\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Per-Class Accuracy')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.savefig(f'{outputDir}/per_class_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    if len(misclassified_images) > 0:\n",
    "        sorted_indices = np.argsort(misclassified_confidences)[::-1]\n",
    "        top_confident_misclassifications = [(misclassified_labels[i],\n",
    "                                             misclassified_predictions[i],\n",
    "                                             misclassified_confidences[i],\n",
    "                                             misclassified_images[i])\n",
    "                                            for i in sorted_indices[:20]]\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=4, ncols=5, figsize=(15, 12))\n",
    "        fig.suptitle('Top 20 Most Confident Misclassifications', fontsize=16)\n",
    "\n",
    "        for i, (label, pred, conf, img) in enumerate(top_confident_misclassifications):\n",
    "            if i < 20:\n",
    "                ax = axes[i // 5, i % 5]\n",
    "                img = denormalize(img)\n",
    "                img = np.transpose(img.numpy(), (1, 2, 0))\n",
    "                img = np.clip(img, 0, 1)\n",
    "                ax.imshow(img)\n",
    "                ax.set_title(f'True: {classes[label]}\\nPred: {classes[pred]}\\nConf: {conf:.3f}')\n",
    "                ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        plt.savefig(f'{outputDir}/top_confident_misclassifications.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    print(f'Additional visualizations saved to {outputDir}/')\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'misclassified_images': misclassified_images,\n",
    "        'misclassified_predictions': misclassified_predictions,\n",
    "        'misclassified_labels': misclassified_labels,\n",
    "        'misclassified_confidences': misclassified_confidences,\n",
    "        'classes': classes\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    modelPath = f'{DRIVE_CHECKPOINT_DIR}/resnet152_best.pt'\n",
    "    outputDir = '/content/drive/MyDrive/CIFAR10-model5-testing'\n",
    "\n",
    "    evaluate_model(modelPath, outputDir)"
   ],
   "metadata": {
    "id": "fzOQkzvnbaZY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "FINETUNE_CHECKPOINT_DIR = '/content/drive/MyDrive/CIFAR10-model5-finetuning'\n",
    "\n",
    "\n",
    "def get_data_loaders(batch_size=64, num_workers=2):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    return trainloader, testloader, classes\n",
    "\n",
    "# Mixup data augmentation\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def train(model, trainloader, criterion, optimizer, scheduler, epoch, use_mixup=True):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loop = tqdm(trainloader, desc=f\"Fine-tune Epoch {epoch + 1}\")\n",
    "    for batch_idx, (inputs, targets) in enumerate(loop):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        if use_mixup:\n",
    "            inputs, targets_a, targets_b, lam = mixup_data(inputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += (lam * predicted.eq(targets_a).sum().item() +\n",
    "                       (1 - lam) * predicted.eq(targets_b).sum().item())\n",
    "        else:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        loop.set_postfix(loss=train_loss / (batch_idx + 1), acc=100. * correct / total)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(trainloader), 100.0 * correct / total\n",
    "\n",
    "def val(model, testloader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in testloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return test_loss / len(testloader), 100.0 * correct / total\n",
    "\n",
    "def main():\n",
    "    num_epochs = 20\n",
    "    batch_size = 64\n",
    "    lr = 0.001\n",
    "\n",
    "    os.makedirs('finetune_results', exist_ok=True)\n",
    "\n",
    "    trainloader, testloader, classes = get_data_loaders(batch_size)\n",
    "\n",
    "    model = ResNet152().to(device)\n",
    "    print(f\"Created ResNet152 model with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "\n",
    "    best_model_path = f'{FINETUNE_CHECKPOINT_DIR}/resnet152_finetune_best2.pt'\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    print(f\"Loaded best model weights from {best_model_path}\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    best_acc = 0\n",
    "\n",
    "    initial_test_loss, initial_test_acc = val(model, testloader, criterion)\n",
    "    print(f\"Before fine-tuning - Test Loss: {initial_test_loss:.4f}, Test Acc: {initial_test_acc:.2f}%\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        use_mixup = (epoch < num_epochs - 2)\n",
    "        train_loss, train_acc = train(model, trainloader, criterion, optimizer, scheduler, epoch, use_mixup)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        test_loss, test_acc = val(model, testloader, criterion)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "        print(f'\\nEpoch {epoch + 1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "        print(f'Learning rate: {scheduler.get_last_lr()[0]:.6f}')\n",
    "\n",
    "        drive_checkpoint_path = f'{FINETUNE_CHECKPOINT_DIR}/resnet152_finetune_epoch_{epoch+1}.pt'\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'test_loss': test_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'test_acc': test_acc\n",
    "        }, drive_checkpoint_path)\n",
    "        print(f'Checkpoint saved to Google Drive: resnet152_finetune_epoch_{epoch+1}.pt')\n",
    "\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            torch.save(model.state_dict(), 'finetune_results/resnet152_finetune_best.pt')\n",
    "\n",
    "            best_drive_path = f'{FINETUNE_CHECKPOINT_DIR}/resnet152_finetune_best.pt'\n",
    "            torch.save(model.state_dict(), best_drive_path)\n",
    "            print(f'New best model saved! (Accuracy: {test_acc:.2f}%)')\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.title('Loss vs. Epochs (Fine-tuning)')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Train Accuracy')\n",
    "    plt.plot(test_accs, label='Test Accuracy')\n",
    "    plt.title('Accuracy vs. Epochs (Fine-tuning)')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('finetune_results/finetuning_curves.png')\n",
    "    plt.savefig(f'{FINETUNE_CHECKPOINT_DIR}/finetuning_curves.png')\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Initial accuracy before fine-tuning: {initial_test_acc:.2f}%')\n",
    "    print(f'Best accuracy after fine-tuning: {best_acc:.2f}%')\n",
    "    print(f'Improvement: {best_acc - initial_test_acc:.2f}%')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "  # 2 is old model"
   ],
   "metadata": {
    "id": "KpD8DTyzbbTH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_model(modelPath, outputDir='./', max_examples=100):\n",
    "\n",
    "    if not os.path.exists(outputDir):\n",
    "        os.makedirs(outputDir)\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    model = ResNet152()\n",
    "\n",
    "    state_dict = torch.load(modelPath, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    misclassified_images = []\n",
    "    misclassified_predictions = []\n",
    "    misclassified_labels = []\n",
    "    misclassified_confidences = []\n",
    "\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(testloader, desc='Evaluating'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "            misclassified_indices = (predicted != labels).nonzero(as_tuple=True)[0]\n",
    "\n",
    "            for idx in misclassified_indices:\n",
    "                if len(misclassified_images) < max_examples:\n",
    "                    misclassified_images.append(images[idx].cpu())\n",
    "                    misclassified_predictions.append(predicted[idx].item())\n",
    "                    misclassified_labels.append(labels[idx].item())\n",
    "\n",
    "                    softmax_probs = torch.nn.functional.softmax(outputs[idx], dim=0)\n",
    "                    confidence = softmax_probs[predicted[idx]].item()\n",
    "                    misclassified_confidences.append(confidence)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    print(f'Saving {len(misclassified_images)} misclassified examples to {outputDir}...')\n",
    "\n",
    "    with open(f'{outputDir}/misclassifications_summary.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(f'Total accuracy: {accuracy:.2f}%\\n')\n",
    "        f.write(f'Total misclassifications: {total - correct} out of {total}\\n\\n')\n",
    "        f.write('Class-wise misclassifications:\\n')\n",
    "\n",
    "        class_correct = [0] * 10\n",
    "        class_total = [0] * 10\n",
    "\n",
    "        for i in range(len(all_targets)):\n",
    "            label = all_targets[i]\n",
    "            class_total[label] += 1\n",
    "            if all_predictions[i] == label:\n",
    "                class_correct[label] += 1\n",
    "\n",
    "        for i in range(10):\n",
    "            class_acc = 100 * class_correct[i] / class_total[i]\n",
    "            f.write(f'{classes[i]}: {class_acc:.2f}% correct ({class_correct[i]} out of {class_total[i]})\\n')\n",
    "\n",
    "        f.write('\\nTop confusion pairs (true → predicted):\\n')\n",
    "        confusion = {}\n",
    "        for true, pred in zip(all_targets, all_predictions):\n",
    "            if true != pred:\n",
    "                key = (true, pred)\n",
    "                confusion[key] = confusion.get(key, 0) + 1\n",
    "                confusion[key] = confusion.get(key, 0) + 1\n",
    "\n",
    "        top_confusions = sorted(confusion.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        for (true, pred), count in top_confusions:\n",
    "            f.write(f'{classes[true]} → {classes[pred]}: {count} times\\n')\n",
    "\n",
    "    ncols = 5\n",
    "    nrows = min(len(misclassified_images) // ncols + (1 if len(misclassified_images) % ncols != 0 else 0), 20)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 3 * nrows))\n",
    "    fig.suptitle(f'Misclassified Examples (showing {nrows * ncols} out of {len(misclassified_images)})', fontsize=16)\n",
    "\n",
    "    def denormalize(tensor):\n",
    "        mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)\n",
    "        return tensor * std + mean\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < len(misclassified_images):\n",
    "            # Denormalize the image\n",
    "            img = denormalize(misclassified_images[i])\n",
    "            img = np.transpose(img.numpy(), (1, 2, 0))  # Convert to HWC format\n",
    "            img = np.clip(img, 0, 1)  # Ensure values are between 0 and 1\n",
    "\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(\n",
    "                f'True: {classes[misclassified_labels[i]]}\\nPred: {classes[misclassified_predictions[i]]}\\nConf: {misclassified_confidences[i]:.2f}')\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.savefig(f'{outputDir}/misclassified_examples.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    print(f'Misclassifications summary saved to {outputDir}/misclassifications_summary.txt')\n",
    "    print(f'Misclassified examples grid saved to {outputDir}/misclassified_examples.png')\n",
    "\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "\n",
    "    cm = confusion_matrix(all_targets, all_predictions)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(f'{outputDir}/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    class_accuracies = [100 * class_correct[i] / class_total[i] for i in range(10)]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(classes, class_accuracies)\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Per-Class Accuracy')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.savefig(f'{outputDir}/per_class_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    if len(misclassified_images) > 0:\n",
    "        sorted_indices = np.argsort(misclassified_confidences)[::-1]\n",
    "        top_confident_misclassifications = [(misclassified_labels[i],\n",
    "                                             misclassified_predictions[i],\n",
    "                                             misclassified_confidences[i],\n",
    "                                             misclassified_images[i])\n",
    "                                            for i in sorted_indices[:20]]\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=4, ncols=5, figsize=(15, 12))\n",
    "        fig.suptitle('Top 20 Most Confident Misclassifications', fontsize=16)\n",
    "\n",
    "        for i, (label, pred, conf, img) in enumerate(top_confident_misclassifications):\n",
    "            if i < 20:\n",
    "                ax = axes[i // 5, i % 5]\n",
    "                img = denormalize(img)\n",
    "                img = np.transpose(img.numpy(), (1, 2, 0))\n",
    "                img = np.clip(img, 0, 1)\n",
    "                ax.imshow(img)\n",
    "                ax.set_title(f'True: {classes[label]}\\nPred: {classes[pred]}\\nConf: {conf:.3f}')\n",
    "                ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        plt.savefig(f'{outputDir}/top_confident_misclassifications.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    print(f'Additional visualizations saved to {outputDir}/')\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'misclassified_images': misclassified_images,\n",
    "        'misclassified_predictions': misclassified_predictions,\n",
    "        'misclassified_labels': misclassified_labels,\n",
    "        'misclassified_confidences': misclassified_confidences,\n",
    "        'classes': classes\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    modelPath = f'{DRIVE_CHECKPOINT_DIR}/resnet152_best.pt'\n",
    "    outputDir = './misclassifications'\n",
    "\n",
    "    evaluate_model(modelPath, outputDir)"
   ],
   "metadata": {
    "id": "vhFNyex-bcjD"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
